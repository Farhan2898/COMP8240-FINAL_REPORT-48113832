{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farhan2898/COMP8240-FINAL_REPORT-48113832/blob/main/Report_48113832_MD_FARHAN_TANVIR_COMP8240.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "id": "H39wTzh34nf-",
        "outputId": "cf511244-5e06-46f7-8c18-42ac1a8334f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296189 sha256=0f918862592f08fd06e2f88a46fb7635fb16a05b7df1b7dbbc156411868eb89b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "D4w0nF-18bSY",
        "outputId": "728c33c2-2c7f-48a0-99ab-96862c65a611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YELPP DATASET**"
      ],
      "metadata": {
        "id": "qCikM3p1mVjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "# Download NLTK stopwords and wordnet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the Yelp training and test datasets\n",
        "yelpp_train_df = pd.read_csv('train.csv')\n",
        "yelpp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Adjust column names based on the structure of your dataset\n",
        "yelpp_train_df.columns = ['label', 'text']\n",
        "yelpp_test_df.columns = ['label', 'text']\n",
        "\n",
        "# Get the set of stopwords for removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Precompile regex patterns for efficiency\n",
        "url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\n",
        "special_char_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "extra_space_pattern = re.compile(r'\\s+')\n",
        "\n",
        "def fast_preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = url_pattern.sub('', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = special_char_pattern.sub('', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = extra_space_pattern.sub(' ', text).strip()\n",
        "\n",
        "    # Remove stop words in a vectorized way\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join back into a string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing using vectorized operations\n",
        "yelpp_train_df['text'] = yelpp_train_df['text'].map(fast_preprocess)\n",
        "yelpp_test_df['text'] = yelpp_test_df['text'].map(fast_preprocess)\n",
        "\n",
        "# Prepare the training data for FastText\n",
        "yelpp_train_df['fasttext_format'] = yelpp_train_df['label'].apply(lambda x: f'__label__{x}') + ' ' + yelpp_train_df['text']\n",
        "preprocessed_yelpp_train_file = 'yelpp_train_fasttext.txt'\n",
        "yelpp_train_df['fasttext_format'].to_csv(preprocessed_yelpp_train_file, index=False, header=False)\n",
        "\n",
        "# Prepare the test data for FastText\n",
        "yelpp_test_df['fasttext_format'] = yelpp_test_df['label'].apply(lambda x: f'__label__{x}') + ' ' + yelpp_test_df['text']\n",
        "preprocessed_yelpp_test_file = 'yelpp_test_fasttext.txt'\n",
        "yelpp_test_df['fasttext_format'].to_csv(preprocessed_yelpp_test_file, index=False, header=False)\n",
        "\n",
        "print(\"\\nOptimized preprocessing complete. Data saved for FastText.\")"
      ],
      "metadata": {
        "id": "nuBa1L5A8m_m",
        "outputId": "e0f8f388-8d78-4fae-a90a-cf3284464ad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimized preprocessing complete. Data saved for FastText.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FASTTEXT MODEL FOR YELPP WITHOUT BIGRAM Using Different Hypertuning**"
      ],
      "metadata": {
        "id": "Zn7WK8jsmh8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.05, epoch=10,dim=10,thread=90**"
      ],
      "metadata": {
        "id": "c1GtFkNHncG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Define file paths for training and test data\n",
        "preprocessed_yelpp_train_file = 'yelpp_train_fasttext.txt'\n",
        "preprocessed_yelpp_test_file = 'yelpp_test_fasttext.txt'\n",
        "\n",
        "# Train the FastText model\n",
        "yelp_model = fasttext.train_supervised(\n",
        "    input=preprocessed_yelpp_train_file,\n",
        "    lr=0.05,           # Initial learning rate\n",
        "    epoch=10,         # Number of epochs\n",
        "    dim=10,           # Set h=10\n",
        "    thread=90\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "yelp_model.save_model('fasttext_yelpp_model.bin')\n",
        "print(\"\\nModel training complete. Model saved as 'fasttext_yelpp_model.bin'.\")"
      ],
      "metadata": {
        "id": "g2-XQ-8n86wQ",
        "outputId": "ff2d8a3e-9233-43fa-97c7-9339798ee11a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete. Model saved as 'fasttext_yelpp_model.bin'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY When lr=0.05, epoch=10,dim=10,thread=90. ACCURACY 93.10%**"
      ],
      "metadata": {
        "id": "cJ_9AudynDAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = yelp_model.test(preprocessed_yelpp_test_file)\n",
        "\n",
        "# Print the results\n",
        "print(f'\\nTest Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd27OXfxM6Xv",
        "outputId": "210d5ff1-741d-4c02-dc38-a17513f2d3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Precision: 93.10%\n",
            "Test Recall: 93.10%\n",
            "Number of test examples:37999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.05, epoch=50,dim=10,thread=90**"
      ],
      "metadata": {
        "id": "h8pHXb_GnUXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Define file paths for training and test data\n",
        "preprocessed_yelpp_train_file = 'yelpp_train_fasttext.txt'\n",
        "preprocessed_yelpp_test_file = 'yelpp_test_fasttext.txt'\n",
        "\n",
        "# Train the FastText model\n",
        "yelp_model = fasttext.train_supervised(\n",
        "    input=preprocessed_yelpp_train_file,\n",
        "    lr=0.05,           # Initial learning rate\n",
        "    epoch=50,         # Number of epochs\n",
        "    dim=10,           # Set h=10\n",
        "    thread=90\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "yelp_model.save_model('fasttext_yelpp_model.bin')\n",
        "print(\"\\nModel training complete. Model saved as 'fasttext_yelpp_model.bin'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQOjYPgiNR5y",
        "outputId": "75961a86-2cbb-4a28-efa4-e48f90cfb248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete. Model saved as 'fasttext_yelpp_model.bin'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY When lr=0.05, epoch=50,dim=10,thread=90. ACCURACY 92.49%**"
      ],
      "metadata": {
        "id": "721rcXD6oFzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = yelp_model.test(preprocessed_yelpp_test_file)\n",
        "\n",
        "# Print the results\n",
        "print(f'\\nTest Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAASbRNSNWG_",
        "outputId": "be5f638c-0455-43d8-f70e-323b893c03d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Precision: 92.49%\n",
            "Test Recall: 92.49%\n",
            "Number of test examples:37999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FASTTEXT MODEL FOR YELPP WITH BIGRAM Using Different Hypertuning**"
      ],
      "metadata": {
        "id": "tgI45xQ5oW4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.05,wordNgrams=2, epoch=10,dim=10,thread=90**"
      ],
      "metadata": {
        "id": "D8DAMfQWo8qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Define file paths for training and test data\n",
        "preprocessed_yelpp_train_file = 'yelpp_train_fasttext.txt'\n",
        "preprocessed_yelpp_test_file = 'yelpp_test_fasttext.txt'\n",
        "\n",
        "# Train the FastText model\n",
        "yelp_model = fasttext.train_supervised(\n",
        "    input=preprocessed_yelpp_train_file,\n",
        "    lr=0.05,           # Initial learning rate\n",
        "    epoch=10,         # Number of epochs\n",
        "    wordNgrams=2,     # Use bigrams\n",
        "    dim=10,           # Set h=10\n",
        "    thread=90\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "yelp_model.save_model('fasttext_yelpp_model.bin')\n",
        "print(\"\\nModel training complete. Model saved as 'fasttext_yelpp_model.bin'.\")"
      ],
      "metadata": {
        "id": "pQltu87vDIz6",
        "outputId": "6638650d-b2b0-4df8-89a1-29b9ab9068eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete. Model saved as 'fasttext_yelpp_model.bin'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PREPROCESSED YELPP DATASET**"
      ],
      "metadata": {
        "id": "7M2cjD4Xqkvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(preprocessed_yelpp_test_file, 'r') as file:\n",
        "    for i in range(5):\n",
        "        print(file.readline())"
      ],
      "metadata": {
        "id": "TyrBUAhJCK73",
        "outputId": "a06d7504-ca6a-4cb3-d248-f8c9d81aa70e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_label_1 last summer appointment get new tires wait super long time also went week fix minor problem tire put fixed free next morning issue called complain manager didnt even apologize frustrated never going back seem overpriced\n",
            "\n",
            "_label_2 friendly staff starbucks fair get anywhere else sometimes lines get long\n",
            "\n",
            "_label_1 food good unfortunately service hit miss main issue seems kitchen waiters waitresses often apologetic long waits pretty obvious avoid tables taking initial order avoid hearing complaints\n",
            "\n",
            "_label_2 even didnt car filenes basement worth bus trip waterfront always find something usually find things spend better still always still wearing clothes shoes months later nni kind suspect best shopping pittsburgh much better usual department stores better marshalls tj maxx better saks downtown even sale selection bargains qualitynni like filenes better gabriel brothers harder get gabriel brothers real discount shoppers challenge im afraid didnt live pittsburgh long enough develop necessary skills filenes still running june left town\n",
            "\n",
            "_label_2 picture billy joels piano man doubled mixed beer rowdy crowd comedy welcome sing sing unique musical experience found homesteadnnif youre looking grab bite eat beer come serving food brews rock bottom brewery sing sing keeps tummy full listen two amazingly talented pianists take musical requests theyll play anything youd like tips course wanting hear britney spears toto duran duran yep play new oldnnthe crowd makes show make sure come ready good time crowd dead harder guys get reaction youre wanting fun great time perfect place birthday parties especially want embarrass friend guys bring pianos perform little ditty good sport get coveted sing sing bumper sticker wouldnt want thatnndueling pianos brews time shut sing sing\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Accuracy with lr=0.05,wordNgrams=2, epoch=10,dim=10,thread=90  ACCURACY=94.43%**"
      ],
      "metadata": {
        "id": "R5CGrS7DpTTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = yelp_model.test(preprocessed_yelpp_test_file)\n",
        "\n",
        "# Print the results\n",
        "print(f'\\nTest Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "id": "xxChibAQ9MFK",
        "outputId": "cfc11acf-5f14-404c-c5f7-8a113f868480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Precision: 94.43%\n",
            "Test Recall: 94.43%\n",
            "Number of test examples:37999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.05,wordNgrams=2, epoch=50,dim=10,thread=90**"
      ],
      "metadata": {
        "id": "rVcgKOuRpjcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Define file paths for training and test data\n",
        "preprocessed_yelpp_train_file = 'yelpp_train_fasttext.txt'\n",
        "preprocessed_yelpp_test_file = 'yelpp_test_fasttext.txt'\n",
        "\n",
        "# Train the FastText model\n",
        "yelp_model = fasttext.train_supervised(\n",
        "    input=preprocessed_yelpp_train_file,\n",
        "    lr=0.1,           # Initial learning rate\n",
        "    epoch=50,         # Number of epochs\n",
        "    wordNgrams=2,     # Use bigrams\n",
        "    dim=10,           # Set h=10\n",
        "    thread=90\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "yelp_model.save_model('fasttext_yelpp_model.bin')\n",
        "print(\"\\nModel training complete. Model saved as 'fasttext_yelpp_model.bin'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHTD0ddl8alN",
        "outputId": "5cad0e8a-20b0-4f38-fe4f-ee7d5fbc8688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete. Model saved as 'fasttext_yelpp_model.bin'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY With lr=0.05,wordNgrams=2, epoch=50,dim=10,thread=90 ACCURACY=94.15%**"
      ],
      "metadata": {
        "id": "kf1s2INtprHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = yelp_model.test(preprocessed_yelpp_test_file)\n",
        "\n",
        "# Print the results\n",
        "print(f'\\nTest Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8dyDy3t8iA9",
        "outputId": "68254fab-b378-49fe-fd0e-f70169379afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Precision: 94.15%\n",
            "Test Recall: 94.15%\n",
            "Number of test examples:37999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnYniAd2-lUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AMAZF DATASET**"
      ],
      "metadata": {
        "id": "6hI6aDHTr8Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords if not already downloaded\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "# Get the total number of rows in the CSV file\n",
        "total_rows = sum(1 for row in open('Amaztrain.csv', 'r', encoding='utf-8'))\n",
        "\n",
        "# Specify the number of rows to read, excluding the last 5\n",
        "rows_to_read = total_rows-5\n",
        "\n",
        "# Load the Amazon training and test datasets\n",
        "amazf_train_df = pd.read_csv('Amaztrain.csv', header=None, nrows=rows_to_read)\n",
        "amazf_test_df = pd.read_csv('Amaztest.csv', header=None)\n",
        "\n",
        "\n",
        "# Rename the columns to 'label' and 'text'\n",
        "amazf_train_df.columns = ['label', 'title', 'text']\n",
        "amazf_test_df.columns = ['label', 'title', 'text']\n",
        "\n",
        "# Select only 'label' and 'text' columns\n",
        "amazf_train_df = amazf_train_df[['label', 'text']]\n",
        "amazf_test_df = amazf_test_df[['label','text']]\n",
        "\n",
        "\n",
        "\n",
        "# Get the set of stopwords for removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Precompile regex patterns for efficiency\n",
        "url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\n",
        "special_char_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "extra_space_pattern = re.compile(r'\\s+')\n",
        "\n",
        "def fast_preprocess(text):\n",
        "    # Check if the input is a string, otherwise convert it to an empty string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = url_pattern.sub('', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = special_char_pattern.sub('', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = extra_space_pattern.sub(' ', text).strip()\n",
        "\n",
        "    # Remove stop words\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join back into a string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing using vectorized operations\n",
        "amazf_train_df['text'] = amazf_train_df['text'].map(fast_preprocess)\n",
        "amazf_test_df['text'] = amazf_test_df['text'].map(fast_preprocess)\n",
        "\n",
        "# Prepare the training data for FastText\n",
        "amazf_train_df['fasttext_format'] = amazf_train_df['label'].apply(lambda x: f'__label__{x}') + ' ' + amazf_train_df['text']\n",
        "preprocessed_amazf_train_file = 'amazf_train_fasttext.txt'\n",
        "amazf_train_df['fasttext_format'].to_csv(preprocessed_amazf_train_file, index=False, header=False)\n",
        "\n",
        "# Prepare the test data for FastText\n",
        "amazf_test_df['fasttext_format'] = amazf_test_df['label'].apply(lambda x: f'__label__{x}') + ' ' + amazf_test_df['text']\n",
        "preprocessed_amazf_test_file = 'amazf_test_fasttext.txt'\n",
        "amazf_test_df['fasttext_format'].to_csv(preprocessed_amazf_test_file, index=False, header=False)\n",
        "\n",
        "print(\"\\nOptimized preprocessing complete. Data saved for FastText.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WVwHvA1P8ce",
        "outputId": "befceb51-313d-4a2c-bdb5-7292940fdbe0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimized preprocessing complete. Data saved for FastText.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FASTTEXT MODEL FOR AMAZF WITHOUT BIGRAM Using Different Hypertuning**"
      ],
      "metadata": {
        "id": "0WEZJtEFsI1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.1, epoch=10,dim=10,thread=90**"
      ],
      "metadata": {
        "id": "bOLa1mMlsQa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "amazf_model = fasttext.train_supervised(\n",
        "    input=preprocessed_amazf_train_file,\n",
        "    lr=0.1,              # Further lower learning rate for fine-tuning\n",
        "    epoch=10,            # Increase the number of epochs\n",
        "    dim=10,              # Increase the dimension of embeddings\n",
        "    loss='softmax',       # Softmax loss for better classification\n",
        "    thread=90             # Keep using multiple threads for faster training\n",
        ")\n",
        "\n",
        "# Save the updated model\n",
        "amazf_model.save_model('fasttext_amazf_model_optimized_v2.bin')\n",
        "\n",
        "print(\"\\nFurther optimized model training complete.\")\n"
      ],
      "metadata": {
        "id": "_FDbWu1Rk8Ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92640f55-9d9d-4ea0-f701-26417be3df5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Further optimized model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY With lr=0.1, epoch=10,dim=10,thread=90 ACCURACY=44.61%**"
      ],
      "metadata": {
        "id": "fC8WxGwZsVs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = amazf_model.test(preprocessed_amazf_test_file)\n",
        "\n",
        "# Print results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "id": "YggYRyr6lB_M",
        "outputId": "11328f55-8f86-497d-ee72-32a9995036ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 44.61%\n",
            "Test Recall: 44.61%\n",
            "Number of test examples:650000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.1, epoch=50,dim=10,thread=90**"
      ],
      "metadata": {
        "id": "CtRyEGgKs0Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "amazf_model = fasttext.train_supervised(\n",
        "    input=preprocessed_amazf_train_file,\n",
        "    lr=0.1,              # Further lower learning rate for fine-tuning\n",
        "    epoch=50,            # Increase the number of epochs\n",
        "    dim=10,              # Increase the dimension of embeddings\n",
        "    loss='softmax',       # Softmax loss for better classification\n",
        "    thread=90             # Keep using multiple threads for faster training\n",
        ")\n",
        "\n",
        "# Save the updated model\n",
        "amazf_model.save_model('fasttext_amazf_model_optimized_v2.bin')\n",
        "\n",
        "print(\"\\nFurther optimized model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49c518pXPrlC",
        "outputId": "83bd7366-89ff-4b51-c40a-7649aff8fa0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Further optimized model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY With lr=0.1, epoch=50,dim=10,thread=90 ACCURACY=44.15%**"
      ],
      "metadata": {
        "id": "ANBMlhOFs5H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = amazf_model.test(preprocessed_amazf_test_file)\n",
        "\n",
        "# Print results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YhVGk6LPzaP",
        "outputId": "1151622a-a7f4-4de3-de56-11d30e160860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 44.15%\n",
            "Test Recall: 44.15%\n",
            "Number of test examples:650000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.1, epoch=20,dim=10,thread=90**"
      ],
      "metadata": {
        "id": "_gIKIOwUtS8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "amazf_model = fasttext.train_supervised(\n",
        "    input=preprocessed_amazf_train_file,\n",
        "    lr=0.1,              # Further lower learning rate for fine-tuning\n",
        "    epoch=20,            # Increase the number of epochs\n",
        "    dim=10,              # Increase the dimension of embeddings\n",
        "    loss='softmax',       # Softmax loss for better classification\n",
        "    thread=90,            # Keep using multiple threads for faster training\n",
        "    minn = 2,\n",
        "    maxn= 6\n",
        ")\n",
        "\n",
        "# Save the updated model\n",
        "amazf_model.save_model('fasttext_amazf_model_optimized_v2.bin')\n",
        "\n",
        "print(\"\\nFurther optimized model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrQgQ-_6P8eU",
        "outputId": "90f663e2-ffdc-453d-c156-8d819212336a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Further optimized model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY With lr=0.1, epoch=50,dim=10,thread=90 ACCURACY=50.80%**"
      ],
      "metadata": {
        "id": "dou-DFPhtvap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = amazf_model.test(preprocessed_amazf_test_file)\n",
        "\n",
        "# Print results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BAYebn9SMs6",
        "outputId": "12f4ed4f-db47-424a-b1b6-36746635f2d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 50.80%\n",
            "Test Recall: 50.80%\n",
            "Number of test examples:650000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hunw0hXZtnBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1WhAEq15tnEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FASTTEXT MODEL FOR AMAZF WITH BIGRAM Using Different Hypertuning**"
      ],
      "metadata": {
        "id": "tEL2TuyJtnG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.1, epoch=10,dim=10,wordNgrams=2,thread=90**"
      ],
      "metadata": {
        "id": "N7rQjQYcuHkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "amazf_model = fasttext.train_supervised(\n",
        "    input=preprocessed_amazf_train_file,\n",
        "    lr=0.1,              # Further lower learning rate for fine-tuning\n",
        "    epoch=10,            # Increase the number of epochs\n",
        "    dim=10,              # Increase the dimension of embeddings\n",
        "    wordNgrams=2,         # Capture more context using n-grams\n",
        "    loss='softmax',       # Softmax loss for better classification\n",
        "    thread=90             # Keep using multiple threads for faster training\n",
        ")\n",
        "\n",
        "# Save the updated model\n",
        "amazf_model.save_model('fasttext_amazf_model_optimized_v2.bin')\n",
        "\n",
        "print(\"\\nFurther optimized model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCxbaDniPYVK",
        "outputId": "717d1454-e070-40d4-82eb-9d081c8c98da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Further optimized model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY With lr=0.1, epoch=50,dim=10,thread=90 ACCURACY=44.49%**"
      ],
      "metadata": {
        "id": "bLMySoIXuImY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = amazf_model.test(preprocessed_amazf_test_file)\n",
        "\n",
        "# Print results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjb8cgHiPa10",
        "outputId": "5fb3aeb5-fe72-4655-d113-4e075e312c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 44.49%\n",
            "Test Recall: 44.49%\n",
            "Number of test examples:650000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.1, epoch=20,dim=10,wordNgrams=2,thread=90**"
      ],
      "metadata": {
        "id": "QaObxrUeuJlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "amazf_model = fasttext.train_supervised(\n",
        "    input=preprocessed_amazf_train_file,\n",
        "    lr=0.1,              # Further lower learning rate for fine-tuning\n",
        "    epoch=20,            # Increase the number of epochs\n",
        "    dim=10,              # Increase the dimension of embeddings\n",
        "    loss='softmax',       # Softmax loss for better classification\n",
        "    wordNgrams=2,\n",
        "    thread=90,            # Keep using multiple threads for faster training\n",
        "    minn = 2,\n",
        "    maxn= 6\n",
        ")\n",
        "\n",
        "# Save the updated model\n",
        "amazf_model.save_model('fasttext_amazf_model_optimized_v2.bin')\n",
        "\n",
        "print(\"\\nFurther optimized model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xf2jrNGVnrL",
        "outputId": "7b49695b-3ed8-424f-b9f0-355288ac822c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Further optimized model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY With lr=0.1, epoch=20,dim=10,thread=90,wordNgrams=2 ACCURACY=52.91%**"
      ],
      "metadata": {
        "id": "ihy4ho29uuv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = amazf_model.test(preprocessed_amazf_test_file)\n",
        "\n",
        "# Print results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO-XCipMV1Uz",
        "outputId": "f74abe21-ab8e-49e7-b483-a31fe88d34bf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 52.91%\n",
            "Test Recall: 52.91%\n",
            "Number of test examples:650000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model build lr=0.05, epoch=10,dim=10,wordNgrams=2,thread=90**"
      ],
      "metadata": {
        "id": "9TbwlGzru6C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "amazf_model = fasttext.train_supervised(\n",
        "    input=preprocessed_amazf_train_file,\n",
        "    lr=0.05,              # Further lower learning rate for fine-tuning\n",
        "    epoch=10,            # Increase the number of epochs\n",
        "    dim=10,              # Increase the dimension of embeddings\n",
        "    wordNgrams=2,         # Capture more context using n-grams\n",
        "    loss='softmax',       # Softmax loss for better classification\n",
        "    thread=90             # Keep using multiple threads for faster training\n",
        ")\n",
        "\n",
        "# Save the updated model\n",
        "amazf_model.save_model('fasttext_amazf_model_optimized_v2.bin')\n",
        "\n",
        "print(\"\\nFurther optimized model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4451ldLbSXPU",
        "outputId": "7bcdcc40-e257-4bad-938b-3450fdb2e533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Further optimized model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY With lr=0.05, epoch=10,dim=10,thread=90,wordNgrams=2 ACCURACY=52.91%**"
      ],
      "metadata": {
        "id": "CtbxkPd7u7SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = amazf_model.test(preprocessed_amazf_test_file)\n",
        "\n",
        "# Print results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d--G0rvdSaYk",
        "outputId": "fa5423b7-fae2-47d8-c6fa-3e826db4d6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 44.82%\n",
            "Test Recall: 44.82%\n",
            "Number of test examples:650000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model build with Hashing Trick where lr=0.05, epoch=10,dim=10,wordNgrams=2,thread=90,minn=2,maxn=6"
      ],
      "metadata": {
        "id": "t-O6xq_-vlCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "amazf_model = fasttext.train_supervised(\n",
        "    input=preprocessed_amazf_train_file,\n",
        "    lr=0.05,              # Further lower learning rate for fine-tuning\n",
        "    epoch=10,            # Increase the number of epochs\n",
        "    dim=10,              # Increase the dimension of embeddings\n",
        "    loss='softmax',       # Softmax loss for better classification\n",
        "    wordNgrams=2,\n",
        "    thread=90,            # Keep using multiple threads for faster training\n",
        "    minn = 2,\n",
        "    maxn= 6\n",
        ")\n",
        "\n",
        "# Save the updated model\n",
        "amazf_model.save_model('fasttext_amazf_model_optimized_v2.bin')\n",
        "\n",
        "print(\"\\nFurther optimized model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyWSf7TiYWMi",
        "outputId": "054e999b-b4e7-41ac-e2ce-a46d894d1144"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Further optimized model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY With lr=0.05, epoch=10,dim=10,thread=90,wordNgrams=2, minn=2, maxn=6 ACCURACY=52.52%**"
      ],
      "metadata": {
        "id": "D7GGln3FwBjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = amazf_model.test(preprocessed_amazf_test_file)\n",
        "\n",
        "# Print results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples:{n_examples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atrRvKSwYb6K",
        "outputId": "b76aeafc-61f4-488d-c27c-086d17cd1f69"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 52.52%\n",
            "Test Recall: 52.52%\n",
            "Number of test examples:650000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model build with Hashing Trick where lr=0.1, epoch=10,dim=10,wordNgrams=2,thread=90,minn=2,maxn=6"
      ],
      "metadata": {
        "id": "jzO5ZoOVwzS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "amazf_model = fasttext.train_supervised(\n",
        "    input=preprocessed_amazf_train_file,\n",
        "    lr=0.1,              # Further lower learning rate for fine-tuning\n",
        "    epoch=10,            # Increase the number of epochs\n",
        "    dim=10,              # Increase the dimension of embeddings\n",
        "    loss='softmax',       # Softmax loss for better classification\n",
        "    wordNgrams=2,\n",
        "    thread=90,            # Keep using multiple threads for faster training\n",
        "    minn = 2,\n",
        "    maxn= 6\n",
        ")\n",
        "\n",
        "# Save the updated model\n",
        "amazf_model.save_model('fasttext_amazf_model_optimized_v2.bin')\n",
        "\n",
        "print(\"\\nFurther optimized model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sil4D3jJZA-E",
        "outputId": "db2d3e4f-16c4-42f3-a8ea-6055b1b17551"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Further optimized model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ACCURACY With lr=0.1, epoch=10,dim=10,thread=90,wordNgrams=2, minn=2, maxn=6 ACCURACY=52.67%**"
      ],
      "metadata": {
        "id": "n7gZTWEsw_NC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_7nRbrVgxJVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NEW DATASET: FAKENEWSNET DATASET**"
      ],
      "metadata": {
        "id": "c2Y5bGhxxJlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import fasttext\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('FakeNewsNet.csv')\n",
        "\n",
        "# Adjust column names: 'real' (0 for fake, 1 for real) and 'title' for the news headline\n",
        "df = df[['real', 'title']]\n",
        "df.columns = ['label', 'text']\n",
        "\n",
        "# Clean the text: remove URLs, special characters, and lowercase everything\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and digits\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    # Remove stopwords\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply text cleaning\n",
        "df['text'] = df['text'].map(clean_text)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare the training data for FastText\n",
        "train_df['fasttext_format'] = train_df['label'].apply(lambda x: f'__label__{x}') + ' ' + train_df['text']\n",
        "preprocessed_train_file = 'fakenews_train_fasttext.txt'\n",
        "train_df['fasttext_format'].to_csv(preprocessed_train_file, index=False, header=False)\n",
        "\n",
        "# Prepare the test data for FastText\n",
        "test_df['fasttext_format'] = test_df['label'].apply(lambda x: f'__label__{x}') + ' ' + test_df['text']\n",
        "preprocessed_test_file = 'fakenews_test_fasttext.txt'\n",
        "test_df['fasttext_format'].to_csv(preprocessed_test_file, index=False, header=False)\n",
        "\n",
        "# Train the FastText model on the training data\n",
        "fasttext_model = fasttext.train_supervised(\n",
        "    input=preprocessed_train_file,\n",
        "    lr=0.1,      # Learning rate\n",
        "    epoch=50,    # Number of epochs\n",
        "    dim=100,     # Embedding dimensions\n",
        "    wordNgrams=2,  # Word n-grams\n",
        "    loss='softmax', # Loss function\n",
        "    thread=4     # Number of threads\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "fasttext_model.save_model('fakenews_fasttext_model.bin')\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "n_train_examples, train_precision, train_recall = fasttext_model.test(preprocessed_train_file)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "n_test_examples, test_precision, test_recall = fasttext_model.test(preprocessed_test_file)\n",
        "\n",
        "# Print the results for training and test sets\n",
        "print(f'Training Set Precision (Accuracy): {train_precision * 100:.2f}%')\n",
        "print(f'Training Set Recall: {train_recall * 100:.2f}%')\n",
        "print(f'Number of training examples: {n_train_examples}')\n",
        "\n",
        "print(f'Test Set Precision (Accuracy): {test_precision * 100:.2f}%')\n",
        "print(f'Test Set Recall: {test_recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_test_examples}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmZXne2McqIG",
        "outputId": "e0a46dba-5092-4b89-ced1-f70ac36d6973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Precision (Accuracy): 99.49%\n",
            "Training Set Recall: 99.49%\n",
            "Number of training examples: 18556\n",
            "Test Set Precision (Accuracy): 83.38%\n",
            "Test Set Recall: 83.38%\n",
            "Number of test examples: 4640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fEa2LF2dl0G",
        "outputId": "a1159845-c00d-45ad-b5b8-5bdaa9659211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import fasttext\n",
        "import nltk  # Ensure this is imported to download resources\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the Porter Stemmer for stemming\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('FakeNewsNet.csv')\n",
        "\n",
        "# Adjust column names: 'real' (0 for fake, 1 for real) and 'title' for the news headline\n",
        "df = df[['real', 'title']]\n",
        "df.columns = ['label', 'text']\n",
        "\n",
        "# Clean the text: remove URLs, special characters, convert numbers, remove short words, and apply stemming\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and words with fewer than 3 characters, apply stemming\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # Rejoin the words into a cleaned string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply text cleaning\n",
        "df['text'] = df['text'].map(clean_text)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare the training data for FastText\n",
        "train_df['fasttext_format'] = train_df['label'].apply(lambda x: f'__label__{x}') + ' ' + train_df['text']\n",
        "preprocessed_train_file = 'fakenews_train_fasttext.txt'\n",
        "train_df['fasttext_format'].to_csv(preprocessed_train_file, index=False, header=False)\n",
        "\n",
        "# Prepare the test data for FastText\n",
        "test_df['fasttext_format'] = test_df['label'].apply(lambda x: f'__label__{x}') + ' ' + test_df['text']\n",
        "preprocessed_test_file = 'fakenews_test_fasttext.txt'\n",
        "test_df['fasttext_format'].to_csv(preprocessed_test_file, index=False, header=False)\n",
        "\n",
        "# Train the FastText model on the training data\n",
        "fasttext_model = fasttext.train_supervised(\n",
        "    input=preprocessed_train_file,\n",
        "    lr=0.05,      # Learning rate\n",
        "    epoch=25,    # Number of epochs\n",
        "    dim=10,     # Embedding dimensions\n",
        "    wordNgrams=2,  # Word n-grams\n",
        "    loss='softmax', # Loss function\n",
        "    thread=90     # Number of threads\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "fasttext_model.save_model('fakenews_fasttext_model.bin')\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "n_train_examples, train_precision, train_recall = fasttext_model.test(preprocessed_train_file)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "n_test_examples, test_precision, test_recall = fasttext_model.test(preprocessed_test_file)\n",
        "\n",
        "# Print the results for training and test sets\n",
        "print(f'Training Set Precision (Accuracy): {train_precision * 100:.2f}%')\n",
        "print(f'Training Set Recall: {train_recall * 100:.2f}%')\n",
        "print(f'Number of training examples: {n_train_examples}')\n",
        "\n",
        "print(f'Test Set Precision (Accuracy): {test_precision * 100:.2f}%')\n",
        "print(f'Test Set Recall: {test_recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_test_examples}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx3J9Du0db5u",
        "outputId": "d9b280f6-46a5-46d4-a86f-fcb549e22a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Precision (Accuracy): 99.11%\n",
            "Training Set Recall: 99.11%\n",
            "Number of training examples: 18556\n",
            "Test Set Precision (Accuracy): 83.64%\n",
            "Test Set Recall: 83.64%\n",
            "Number of test examples: 4640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import fasttext\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the Porter Stemmer for stemming\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('FakeNewsNet.csv')\n",
        "\n",
        "# Adjust column names: 'real' (0 for fake, 1 for real) and 'title' for the news headline\n",
        "df = df[['real', 'title']]\n",
        "df.columns = ['label', 'text']\n",
        "\n",
        "# Clean the text: remove URLs, special characters, convert numbers, remove short words, and apply stemming\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and words with fewer than 3 characters, apply stemming\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # Rejoin the words into a cleaned string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply text cleaning\n",
        "df['text'] = df['text'].map(clean_text)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare the training data for FastText\n",
        "train_df['fasttext_format'] = train_df['label'].apply(lambda x: f'__label__{x}') + ' ' + train_df['text']\n",
        "preprocessed_train_file = 'fakenews_train_fasttext.txt'\n",
        "train_df['fasttext_format'].to_csv(preprocessed_train_file, index=False, header=False)\n",
        "\n",
        "# Prepare the test data for FastText\n",
        "test_df['fasttext_format'] = test_df['label'].apply(lambda x: f'__label__{x}') + ' ' + test_df['text']\n",
        "preprocessed_test_file = 'fakenews_test_fasttext.txt'\n",
        "test_df['fasttext_format'].to_csv(preprocessed_test_file, index=False, header=False)\n",
        "\n",
        "# Train the FastText model with more configurations\n",
        "fasttext_model = fasttext.train_supervised(\n",
        "    input=preprocessed_train_file,\n",
        "    lr=0.05,              # Lower learning rate for more refined learning\n",
        "    epoch=100,            # More epochs to go over data multiple times\n",
        "    dim=300,              # Higher dimension for richer word vectors\n",
        "    wordNgrams=3,         # Capture more context with trigrams\n",
        "    loss='ova',           # Switch to one-vs-all loss function\n",
        "    bucket=2000000,       # Use larger bucket size to handle more n-grams\n",
        "    minCount=2,           # Ignore words that appear fewer than 2 times\n",
        "    thread=8              # Increase threads to speed up training\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "fasttext_model.save_model('fakenews_fasttext_model.bin')\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "n_train_examples, train_precision, train_recall = fasttext_model.test(preprocessed_train_file)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "n_test_examples, test_precision, test_recall = fasttext_model.test(preprocessed_test_file)\n",
        "\n",
        "# Print the results for training and test sets\n",
        "print(f'Training Set Precision (Accuracy): {train_precision * 100:.2f}%')\n",
        "print(f'Training Set Recall: {train_recall * 100:.2f}%')\n",
        "print(f'Number of training examples: {n_train_examples}')\n",
        "\n",
        "print(f'Test Set Precision (Accuracy): {test_precision * 100:.2f}%')\n",
        "print(f'Test Set Recall: {test_recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_test_examples}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JByO_ZYVeyvQ",
        "outputId": "5b758517-36ad-40eb-a228-fcad39aa05f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Precision (Accuracy): 99.49%\n",
            "Training Set Recall: 99.49%\n",
            "Number of training examples: 18556\n",
            "Test Set Precision (Accuracy): 83.66%\n",
            "Test Set Recall: 83.66%\n",
            "Number of test examples: 4640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import fasttext\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "\n",
        "# Initialize the Porter Stemmer for stemming\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('FakeNewsNet.csv')\n",
        "\n",
        "# Adjust column names: 'real' (0 for fake, 1 for real) and 'title' for the news headline\n",
        "df = df[['real', 'title']]\n",
        "df.columns = ['label', 'text']\n",
        "\n",
        "# Remove empty texts or rows with NaN\n",
        "df = df.dropna(subset=['text', 'label'])\n",
        "df = df[df['text'].str.strip() != '']  # Remove rows with empty text fields\n",
        "\n",
        "# Clean the text: remove URLs, special characters, convert numbers, remove short words, and apply stemming\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and words with fewer than 3 characters, apply stemming\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # Rejoin the words into a cleaned string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply text cleaning\n",
        "df['text'] = df['text'].map(clean_text)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "print (\"Preprocessed Data:\")\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHoQPTSzhdvy",
        "outputId": "a81d47f0-bece-4ca3-ccfb-28af5525b9f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Data:\n",
            "       label                                               text\n",
            "9198       0  chuck schumer steami affair high school cheerl...\n",
            "14985      1  trace elli ross shut babi question tell peopl ...\n",
            "15100      1                       friend music parodi broadway\n",
            "6186       1                        jurass world fallen kingdom\n",
            "11038      1  cassi join rank kim kardashian pat mcgrath new...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import fasttext\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the Porter Stemmer for stemming\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('FakeNewsNet.csv')\n",
        "\n",
        "# Adjust column names: 'real' (0 for fake, 1 for real) and 'title' for the news headline\n",
        "df = df[['real', 'title']]\n",
        "df.columns = ['label', 'text']\n",
        "\n",
        "# Remove empty texts or rows with NaN\n",
        "df = df.dropna(subset=['text', 'label'])\n",
        "df = df[df['text'].str.strip() != '']  # Remove rows with empty text fields\n",
        "\n",
        "# Clean the text: remove URLs, special characters, convert numbers, remove short words, and apply stemming\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and words with fewer than 3 characters, apply stemming\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # Rejoin the words into a cleaned string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply text cleaning\n",
        "df['text'] = df['text'].map(clean_text)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare the training data for FastText\n",
        "train_df['fasttext_format'] = train_df['label'].apply(lambda x: f'__label__{x}') + ' ' + train_df['text']\n",
        "preprocessed_train_file = 'fakenews_train_fasttext.txt'\n",
        "train_df['fasttext_format'].to_csv(preprocessed_train_file, index=False, header=False)\n",
        "\n",
        "# Prepare the test data for FastText\n",
        "test_df['fasttext_format'] = test_df['label'].apply(lambda x: f'__label__{x}') + ' ' + test_df['text']\n",
        "preprocessed_test_file = 'fakenews_test_fasttext.txt'\n",
        "test_df['fasttext_format'].to_csv(preprocessed_test_file, index=False, header=False)\n",
        "\n",
        "# Function to evaluate the FastText model with different hyperparameters\n",
        "def evaluate_model(lr, epoch, dim, wordNgrams, loss, bucket=2000000, minCount=2):\n",
        "    try:\n",
        "        # Train the FastText model with the provided hyperparameters\n",
        "        fasttext_model = fasttext.train_supervised(\n",
        "            input=preprocessed_train_file,\n",
        "            lr=lr,               # Learning rate\n",
        "            epoch=epoch,         # Number of epochs\n",
        "            dim=dim,             # Embedding dimension size\n",
        "            wordNgrams=wordNgrams,  # N-gram size\n",
        "            loss=loss,           # Loss function\n",
        "            bucket=bucket,       # Bucket size for n-grams\n",
        "            minCount=minCount,   # Minimum word frequency threshold\n",
        "            thread=8             # Number of threads for parallel training\n",
        "        )\n",
        "\n",
        "        # Evaluate on training set\n",
        "        n_train_examples, train_precision, _ = fasttext_model.test(preprocessed_train_file)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        n_test_examples, test_precision, _ = fasttext_model.test(preprocessed_test_file)\n",
        "\n",
        "        # Return precision values\n",
        "        return train_precision, test_precision\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model training: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Hyperparameter grid (you can adjust these ranges as needed)\n",
        "learning_rates = [0.1, 0.05]  # Reduced range\n",
        "epochs = [10, 25, 100]\n",
        "dimensions = [10]\n",
        "n_grams = [1, 2, 3 ]\n",
        "loss_functions = ['softmax', 'ova']\n",
        "\n",
        "# Store results for comparison\n",
        "results = []\n",
        "\n",
        "# Loop through the hyperparameter combinations\n",
        "for lr in learning_rates:\n",
        "    for epoch in epochs:\n",
        "        for dim in dimensions:\n",
        "            for n_gram in n_grams:\n",
        "                for loss in loss_functions:\n",
        "                    print(f\"Training model with lr={lr}, epoch={epoch}, dim={dim}, n_gram={n_gram}, loss={loss}\")\n",
        "\n",
        "                    # Evaluate the model with the current set of hyperparameters\n",
        "                    train_prec, test_prec = evaluate_model(lr, epoch, dim, n_gram, loss)\n",
        "\n",
        "                    if train_prec is not None and test_prec is not None:\n",
        "                        # Store the results if valid\n",
        "                        results.append({\n",
        "                            'lr': lr,\n",
        "                            'epoch': epoch,\n",
        "                            'dim': dim,\n",
        "                            'n_gram': n_gram,\n",
        "                            'loss': loss,\n",
        "                            'train_precision': train_prec,\n",
        "                            'test_precision': test_prec\n",
        "                        })\n",
        "\n",
        "# Find the best configuration\n",
        "best_result = max(results, key=lambda x: x['test_precision'])\n",
        "\n",
        "# Print the best hyperparameters and their performance\n",
        "print(\"\\nBest hyperparameters based on test precision:\")\n",
        "print(f\"Learning Rate: {best_result['lr']}\")\n",
        "print(f\"Epoch: {best_result['epoch']}\")\n",
        "print(f\"Dimension: {best_result['dim']}\")\n",
        "print(f\"N-gram: {best_result['n_gram']}\")\n",
        "print(f\"Loss Function: {best_result['loss']}\")\n",
        "print(f\"Training Precision: {best_result['train_precision'] * 100:.2f}%\")\n",
        "print(f\"Test Precision: {best_result['test_precision'] * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d1AOULxfT0U",
        "outputId": "121a9ee2-b72e-4840-c880-5331af223d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with lr=0.1, epoch=10, dim=10, n_gram=1, loss=softmax\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.1, epoch=10, dim=10, n_gram=1, loss=ova\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.1, epoch=10, dim=10, n_gram=2, loss=softmax\n",
            "Training model with lr=0.1, epoch=10, dim=10, n_gram=2, loss=ova\n",
            "Training model with lr=0.1, epoch=10, dim=10, n_gram=3, loss=softmax\n",
            "Training model with lr=0.1, epoch=10, dim=10, n_gram=3, loss=ova\n",
            "Training model with lr=0.1, epoch=25, dim=10, n_gram=1, loss=softmax\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.1, epoch=25, dim=10, n_gram=1, loss=ova\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.1, epoch=25, dim=10, n_gram=2, loss=softmax\n",
            "Training model with lr=0.1, epoch=25, dim=10, n_gram=2, loss=ova\n",
            "Training model with lr=0.1, epoch=25, dim=10, n_gram=3, loss=softmax\n",
            "Training model with lr=0.1, epoch=25, dim=10, n_gram=3, loss=ova\n",
            "Training model with lr=0.1, epoch=100, dim=10, n_gram=1, loss=softmax\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.1, epoch=100, dim=10, n_gram=1, loss=ova\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.1, epoch=100, dim=10, n_gram=2, loss=softmax\n",
            "Training model with lr=0.1, epoch=100, dim=10, n_gram=2, loss=ova\n",
            "Training model with lr=0.1, epoch=100, dim=10, n_gram=3, loss=softmax\n",
            "Training model with lr=0.1, epoch=100, dim=10, n_gram=3, loss=ova\n",
            "Training model with lr=0.05, epoch=10, dim=10, n_gram=1, loss=softmax\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.05, epoch=10, dim=10, n_gram=1, loss=ova\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.05, epoch=10, dim=10, n_gram=2, loss=softmax\n",
            "Training model with lr=0.05, epoch=10, dim=10, n_gram=2, loss=ova\n",
            "Training model with lr=0.05, epoch=10, dim=10, n_gram=3, loss=softmax\n",
            "Training model with lr=0.05, epoch=10, dim=10, n_gram=3, loss=ova\n",
            "Training model with lr=0.05, epoch=25, dim=10, n_gram=1, loss=softmax\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.05, epoch=25, dim=10, n_gram=1, loss=ova\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.05, epoch=25, dim=10, n_gram=2, loss=softmax\n",
            "Training model with lr=0.05, epoch=25, dim=10, n_gram=2, loss=ova\n",
            "Training model with lr=0.05, epoch=25, dim=10, n_gram=3, loss=softmax\n",
            "Training model with lr=0.05, epoch=25, dim=10, n_gram=3, loss=ova\n",
            "Training model with lr=0.05, epoch=100, dim=10, n_gram=1, loss=softmax\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.05, epoch=100, dim=10, n_gram=1, loss=ova\n",
            "Error during model training: Encountered NaN.\n",
            "Training model with lr=0.05, epoch=100, dim=10, n_gram=2, loss=softmax\n",
            "Training model with lr=0.05, epoch=100, dim=10, n_gram=2, loss=ova\n",
            "Training model with lr=0.05, epoch=100, dim=10, n_gram=3, loss=softmax\n",
            "Training model with lr=0.05, epoch=100, dim=10, n_gram=3, loss=ova\n",
            "\n",
            "Best hyperparameters based on test precision:\n",
            "Learning Rate: 0.05\n",
            "Epoch: 100\n",
            "Dimension: 10\n",
            "N-gram: 3\n",
            "Loss Function: ova\n",
            "Training Precision: 99.49%\n",
            "Test Precision: 83.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8M0fg4ieP8SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mkXipQVCtYD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kuZjgNxWtYGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uNy2jCZrtYJH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5-tEf0OP8Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFXQC6UiYZRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9NPH1igYZTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dublFxZpZA1b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}